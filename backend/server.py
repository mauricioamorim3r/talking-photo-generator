from fastapi import FastAPI, APIRouter, HTTPException, UploadFile, File, Form
from fastapi.responses import StreamingResponse, FileResponse
from dotenv import load_dotenv
from starlette.middleware.cors import CORSMiddleware
import os
import logging
from pathlib import Path
from pydantic import BaseModel, Field, ConfigDict
from typing import List, Optional, Literal
import uuid
from datetime import datetime, timezone
import fal_client
from elevenlabs import ElevenLabs
from emergent_wrapper import LlmChat, UserMessage, FileContentWithMimeType
import cloudinary
import cloudinary.uploader
import base64
import io
from PIL import Image
from gradio_client import Client
from database import db as database

ROOT_DIR = Path(__file__).parent
load_dotenv(ROOT_DIR / '.env')

# Configure APIs
fal_key = os.environ.get('FAL_KEY', '')
os.environ['FAL_KEY'] = fal_key

elevenlabs_client = ElevenLabs(api_key=os.environ.get('ELEVENLABS_KEY', ''))

# Backend URL for serving images
BACKEND_URL = os.environ.get('BACKEND_URL', 'http://localhost:8001')

cloudinary.config(
    cloud_name=os.environ.get('CLOUDINARY_CLOUD_NAME', ''),
    api_key=os.environ.get('CLOUDINARY_API_KEY', ''),
    api_secret=os.environ.get('CLOUDINARY_API_SECRET', '')
)

# Create the main app without a prefix
app = FastAPI()

# Create a router with the /api prefix
api_router = APIRouter(prefix="/api")

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ==================== MODELS ====================

class ImageAnalysis(BaseModel):
    model_config = ConfigDict(extra="ignore")
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    image_url: str
    cloudinary_id: Optional[str] = None
    analysis: str
    suggested_model: str
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))

class AudioGeneration(BaseModel):
    model_config = ConfigDict(extra="ignore")
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    audio_url: str
    source: Literal["generated", "uploaded"]
    duration: Optional[float] = None
    text: Optional[str] = None
    voice_id: Optional[str] = None
    voice_settings: Optional[dict] = None
    cost: Optional[float] = None
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))

class VideoGeneration(BaseModel):
    model_config = ConfigDict(extra="ignore")
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    image_id: str
    audio_id: Optional[str] = None
    model: Literal["veo3", "sora2", "wav2lip", "open-sora", "wav2lip-free"]
    mode: Literal["premium", "economico"] = "premium"
    prompt: str
    duration: Optional[float] = None
    cost: float = 0.0
    estimated_cost: float = 0.0
    status: Literal["pending", "processing", "completed", "failed"] = "pending"
    result_url: Optional[str] = None
    error: Optional[str] = None
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))

class TokenUsage(BaseModel):
    model_config = ConfigDict(extra="ignore")
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    service: str
    operation: str
    cost: float
    details: Optional[dict] = None
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))

# ==================== REQUEST/RESPONSE MODELS ====================

class AnalyzeImageRequest(BaseModel):
    image_url: str

class GenerateAudioRequest(BaseModel):
    text: str
    voice_id: str = "cgSgspJ2msm6clMCkdW9"  # Default child voice
    stability: float = 0.5
    similarity_boost: float = 0.75
    speed: float = 1.0
    style: float = 0.0

class GenerateVideoRequest(BaseModel):
    image_url: str
    model: Literal["veo3", "sora2", "wav2lip", "open-sora", "wav2lip-free"]
    mode: Literal["premium", "economico"] = "premium"
    prompt: str
    audio_url: Optional[str] = None
    duration: Optional[int] = 5
    cinematic_settings: Optional[dict] = None

class EstimateCostRequest(BaseModel):
    model: Literal["veo3", "sora2", "wav2lip", "open-sora", "wav2lip-free"]
    mode: Literal["premium", "economico"] = "premium"
    duration: int
    with_audio: bool = False

class GeneratedImage(BaseModel):
    model_config = ConfigDict(extra="ignore")
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    prompt: str
    image_url: str
    cost: float = 0.039  # Nano Banana cost per image
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))

class GenerateImageRequest(BaseModel):
    prompt: str

class VerifyPasswordRequest(BaseModel):
    password: str

class TokenUsageStats(BaseModel):
    total_spent: float
    by_service: dict
    recent_operations: List[TokenUsage]

class APIBalance(BaseModel):
    model_config = ConfigDict(extra="ignore")
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    service: str
    initial_balance: float
    current_balance: float
    last_updated: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))

class UpdateBalanceRequest(BaseModel):
    service: str
    initial_balance: float

# ==================== ROUTES ====================

@api_router.get("/")
async def root():
    return {"message": "Video Generation API"}

@api_router.post("/images/upload")
async def upload_image(file: UploadFile = File(...)):
    """Upload image to Cloudinary"""
    try:
        contents = await file.read()
        
        # Upload to Cloudinary
        result = cloudinary.uploader.upload(
            contents,
            folder="video-gen"
        )
        
        return {
            "success": True,
            "image_url": result['secure_url'],
            "cloudinary_id": result['public_id']
        }
    except Exception as e:
        logger.error(f"Error uploading image: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@api_router.post("/images/analyze")
async def analyze_image(request: AnalyzeImageRequest):
    """Analyze image with Gemini and suggest best model with cinematic prompts"""
    try:
        # Download image
        import requests
        import tempfile
        img_response = requests.get(request.image_url)
        img_data = img_response.content

        # Save temporarily (cross-platform)
        temp_dir = tempfile.gettempdir()
        temp_path = os.path.join(temp_dir, f"{uuid.uuid4()}.jpg")
        with open(temp_path, 'wb') as f:
            f.write(img_data)
        
        # Analyze with Gemini
        chat = LlmChat(
            api_key=os.environ.get('GEMINI_KEY', ''),
            session_id=str(uuid.uuid4()),
            system_message="""VocÃª Ã© um diretor de fotografia especialista em criar prompts cinematogrÃ¡ficos otimizados para Sora 2 e Veo 3.

**ðŸš¨ POLÃTICA DE CONTEÃšDO CRÃTICA ðŸš¨**
NUNCA mencione ou inclua QUALQUER referÃªncia a:
- "manter identidade facial", "preservar caracterÃ­sticas faciais", "fidelidade facial", "nÃ£o alterar rosto"
- "100% da identidade", "expressÃµes faciais devem ser preservadas", "caracterÃ­sticas originais"
- "alta fidelidade", "exatamente como na foto", "sem modificar o rosto"

âŒ ESTES TERMOS VIOLAM A POLÃTICA ANTI-DEEPFAKE E BLOQUEIAM A GERAÃ‡ÃƒO âŒ

Os modelos JÃ mantÃªm a imagem original automaticamente. Descreva apenas a AÃ‡ÃƒO e MOVIMENTO desejados.

**PALAVRAS PROIBIDAS:**
âŒ ameaÃ§ador, violento, ataque, sangue, armas, terror, pÃ¢nico, agressivo, afiado
âœ… USE ESTAS: impressionante, dramÃ¡tico, majestoso, intenso, impactante, surpreendente, admirÃ¡vel

**âš ï¸ CONTEÃšDO SENSÃVEL COM CRIANÃ‡AS:**
Quando a imagem contiver CRIANÃ‡AS (menores de 18 anos):
- EVITE descriÃ§Ãµes de aÃ§Ãµes relacionadas a comida/bebida na boca
- EVITE close-ups extremos do rosto
- USE descriÃ§Ãµes gerais e neutras: "crianÃ§a sorrindo", "brincando", "correndo"
- FOQUE em aÃ§Ãµes seguras: brincar, correr, sorrir, acenar
- NÃƒO descreva detalhes faciais em excesso

---

## ðŸ“½ï¸ TEMPLATE ESPECÃFICO PARA SORA 2

**Quando usar**: Cenas com fÃ­sica realista, movimento de personagens, ambientes detalhados

**Estrutura do Prompt para Sora 2 (7 Camadas):**
```
1. [CENA E AMBIENTE]: ambiente, hora do dia, clima, atmosfera
2. [SUJEITO E AÃ‡ÃƒO]: quem/o que estÃ¡ na cena, movimento principal, emoÃ§Ãµes, ritmo
3. [PHYSICS & MATERIALS]: texturas, interaÃ§Ãµes fÃ­sicas, comportamento de materiais (tecido, pelo, Ã¡gua, poeira)
4. [CINEMATOGRAFIA]: tipo de plano, movimento de cÃ¢mera, lente, framing
5. [ILUMINAÃ‡ÃƒO E COR]: fontes de luz, direÃ§Ã£o, mood, paleta de cores
6. [ÃUDIO]: sons de fala, efeitos sonoros, ambiente, mÃºsica (Sora 2 gera Ã¡udio nativo)
7. [QUALIDADE + EXCLUSÃ•ES]: resoluÃ§Ã£o, estilo, texturas + sem watermarks/artifacts
```

**Exemplo Sora 2 Refinado:**
"Campo de flores silvestres ao amanhecer, nÃ©voa leve sobre o solo, atmosfera serena e mÃ¡gica. Golden retriever correndo com energia natural, orelhas balanÃ§ando ao vento, lÃ­ngua para fora, expressÃ£o alegre, patas tocando o solo com movimento rÃ­tmico. FÃ­sica realista: pelo texturizado movendo-se com o vento, flores balanÃ§ando suavemente, gotas de orvalho visÃ­veis nas pÃ©talas. Medium shot transitando para wide shot, cÃ¢mera acompanha o movimento com dolly suave lateral. Lente 50mm com shallow depth of field, foco no cachorro. IluminaÃ§Ã£o golden hour: luz quente e suave lateral, raios de sol filtrados atravÃ©s das flores criando lens flare natural, sombras longas. Paleta: tons Ã¢mbar e verdes vibrantes. Ãudio sincronizado: sons rÃ­tmicos de patas na grama, respiraÃ§Ã£o energÃ©tica do cachorro, vento suave rustling nas flores, pÃ¡ssaros cantando ao fundo. Filmado em 35mm, cinematic color grading, 4K, texturas ultra-detalhadas de pelo e vegetaÃ§Ã£o. Sem watermarks, sem text overlays, sem artifacts digitais."

---

## ðŸ“½ï¸ TEMPLATE ESPECÃFICO PARA VEO 3

**Quando usar**: ProduÃ§Ã£o cinematogrÃ¡fica de alta qualidade, Ã¡udio sincronizado complexo, motion realism extremo

**Estrutura do Prompt para Veo 3 (Profissional):**
```
1. [AÃ‡ÃƒO PRINCIPAL]: movimento detalhado e especÃ­fico do sujeito, timing, transiÃ§Ãµes
2. [CINEMATIC SHOT]: tipo de plano profissional + movimento de cÃ¢mera tÃ©cnico
3. [LENTE E FOCO]: especificaÃ§Ãµes exatas (focal length, aperture, depth of field)
4. [LIGHTING DESIGN]: setup de iluminaÃ§Ã£o profissional (key, fill, rim, bounce)
5. [COLOR GRADING]: paleta cinematogrÃ¡fica especÃ­fica, mood, look references
6. [AUDIO DESIGN]: camadas de som ambiente, foley, mÃºsica, sincronizaÃ§Ã£o precisa
7. [QUALIDADE & EXCLUSÃ•ES]: hyper-realistic, resoluÃ§Ã£o 4K/8K, estilo de filmagem + sem distraÃ§Ãµes
```

**Exemplo Veo 3 Refinado:**
"Mulher de cabelos longos virando a cabeÃ§a lentamente em 3 segundos para a cÃ¢mera, sorriso surgindo gradualmente frame by frame, olhos brilhando com luz refletida, cabelo fluindo naturalmente com movimento suave e orgÃ¢nico, micro-expressÃµes faciais detalhadas. Close-up cinematogrÃ¡fico, cÃ¢mera estÃ¡tica em tripod profissional com rack focus suave transicionando do fundo desfocado para o rosto nÃ­tido. Shot com lente prime 85mm f/1.4, bokeh cremoso hexagonal no background, shallow depth of field isolando o sujeito. Three-point lighting setup: key light LED suave de 45Â° criando sombras naturais, fill light difuso reduzindo contraste, rim light backlight destacando o contorno do cabelo com halo sutil. Color grading cinematogrÃ¡fico: tons quentes (amber/gold) nas highlights, teal frio nos shadows, contraste mÃ©dio, saturaÃ§Ã£o controlada, look de filme comercial premium. Audio design em camadas: ambiente suave de cafÃ© com conversas distantes a 20%, som sutil de movimento de roupa, respiraÃ§Ã£o natural baixa, mÃºsica instrumental melancÃ³lica de piano ao fundo, tudo sincronizado com o movimento. Hyper-realistic 4K, textura de pele ultra-detalhada com poros visÃ­veis, cabelo com strand definition, filmado em estilo de luxury commercial high-end production. Sem watermarks, sem lower thirds, sem lens flares excessivos, color grading consistente."

---

## ðŸŽ¬ TEMPLATES SIMPLIFICADOS (Modelos EconÃ´micos)

**Para Open-Sora (gratuito):**
"[Sujeito] fazendo [aÃ§Ã£o]. [Tipo de plano]. [IluminaÃ§Ã£o bÃ¡sica]. [Movimento natural]. Qualidade cinematogrÃ¡fica."

**Para Wav2lip (sincronizaÃ§Ã£o labial):**
"[Pessoa] falando diretamente para a cÃ¢mera. Close-up. Boa iluminaÃ§Ã£o. Movimento labial sincronizado. HD quality."

---

## ðŸ“‹ FORMATO DE RESPOSTA JSON

Retorne EXATAMENTE este JSON:
{
  "description": "DescriÃ§Ã£o detalhada do que vocÃª vÃª na imagem",
  "subject_type": "pessoa/animal/objeto/boneco/criatura",
  "has_face": true ou false,
  "composition": "AnÃ¡lise da composiÃ§Ã£o atual da imagem",
  "recommended_model_premium": "sora2" ou "veo3" ou "wav2lip",
  "recommended_model_economico": "open-sora" ou "wav2lip-free",
  "reason_premium": "Justificativa tÃ©cnica da escolha (mencione o modelo especÃ­fico)",
  "reason_economico": "Justificativa da opÃ§Ã£o gratuita",
  "prompt_sora2": "Prompt COMPLETO seguindo o template Sora 2 - SEM menÃ§Ãµes a identidade facial",
  "prompt_veo3": "Prompt COMPLETO seguindo o template Veo 3 - SEM menÃ§Ãµes a identidade facial",
  "prompt_economico": "Prompt simplificado para modelos gratuitos",
  "cinematic_details": {
    "subject_action": "DescriÃ§Ã£o detalhada da aÃ§Ã£o",
    "camera_work": "Plano e movimento de cÃ¢mera",
    "lighting": "Setup de iluminaÃ§Ã£o",
    "audio_design": "Design de Ã¡udio (para modelos premium)",
    "style": "Estilo visual e qualidade"
  },
  "tips": "Dicas para melhor resultado"
}

---

## âœ… EXEMPLO COM CRIANÃ‡A (CONTEÃšDO SENSÃVEL):

âŒ **ERRADO:** "CrianÃ§a levando colher Ã  boca, cafÃ© derretendo na boca, close-up extremo do rosto..."

âœ… **CORRETO:**
```json
{
  "prompt_sora2": "CrianÃ§a sorrindo e brincando em um ambiente seguro e familiar. Medium shot com cÃ¢mera estÃ¡vel. IluminaÃ§Ã£o natural suave. Color grading quente e acolhedor. Ãudio: sons de risadas e ambiente alegre. Filmado em estilo documental de famÃ­lia, 4K.",
  
  "prompt_veo3": "CrianÃ§a em atividade lÃºdica natural, sorrindo espontaneamente. Wide shot cinematogrÃ¡fico mantendo distÃ¢ncia respeitosa. Soft lighting natural. Color grading: tons quentes familiares. Audio design: ambiente domÃ©stico agradÃ¡vel, risadas naturais. Shot em estilo de fotografia de famÃ­lia profissional."
}
```

## âœ… EXEMPLO COMPLETO CORRETO (Gato):

âŒ **ERRADO:** "Gato malhado mantendo 100% da identidade facial original e preservando todas as caracterÃ­sticas faciais com fidelidade..."

âœ… **CORRETO:**
```json
{
  "prompt_sora2": "Gato malhado laranja levantando a cabeÃ§a lentamente, orelhas se movendo em atenÃ§Ã£o, olhos grandes focando diretamente na cÃ¢mera, bigodes tremendo sutilmente. Medium shot com movimento sutil de aproximaÃ§Ã£o da cÃ¢mera. Lente 35mm, foco nÃ­tido no rosto. IluminaÃ§Ã£o natural suave de janela, criando soft shadows. Color grading quente e acolhedor. Ãudio: ronronar suave, pequenos movimentos, som ambiente calmo de casa. Filmado em estilo documental naturalista, 4K, textura detalhada de pelo.",
  
  "prompt_veo3": "Gato sentado olhando para cima com curiosidade, pupilas dilatadas reagindo Ã  luz, movimento sutil de pestanejar, cauda balanÃ§ando levemente ao lado. Cinematic close-up com lente 50mm f/2.0, shallow depth of field isolando o sujeito. Soft key light de 45 graus, fill light natural, rim light destacando o pelo. Color grading: tons naturais com leve warmth, preservando a textura real. Audio design: ambiente de casa silencioso, respiraÃ§Ã£o suave do gato, leve som de movimento, atmosfera calma. Hyper-realistic, texturas de pelo em 4K, shot em estilo de pet commercial profissional."
}
```

**LEMBRE-SE:** Os modelos automaticamente usam a imagem como base. VocÃª sÃ³ precisa descrever o MOVIMENTO e CINEMATOGRAFIA desejados."""
        ).with_model("gemini", "gemini-2.0-flash")
        
        image_file = FileContentWithMimeType(
            file_path=temp_path,
            mime_type="image/jpeg"
        )
        
        user_message = UserMessage(
            text="Analise esta imagem como um diretor de fotografia e sugira prompts cinematogrÃ¡ficos completos para ambos os modos (Premium e EconÃ´mico).",
            file_contents=[image_file]
        )
        
        # Add timeout to Gemini call
        import asyncio
        try:
            response = await asyncio.wait_for(
                chat.send_message(user_message),
                timeout=30.0  # 30 seconds timeout
            )
        except asyncio.TimeoutError:
            logger.error("Gemini analysis timed out")
            # Return a default analysis with new structure
            analysis_data = {
                "description": "Imagem carregada",
                "subject_type": "desconhecido",
                "has_face": False,
                "composition": "AnÃ¡lise nÃ£o disponÃ­vel - use configuraÃ§Ãµes manuais",
                "recommended_model_premium": "sora2",
                "recommended_model_economico": "open-sora",
                "reason_premium": "Sora 2 oferece alta qualidade com fÃ­sica realista e Ã¡udio nativo",
                "reason_economico": "Open-Sora Ã© uma opÃ§Ã£o gratuita confiÃ¡vel",
                "prompt_sora2": "Sujeito em movimento natural e realista, com aÃ§Ã£o suave e orgÃ¢nica. Medium shot com cÃ¢mera estÃ¡tica. Lente 50mm, foco no sujeito. Soft natural light. Color grading cinematogrÃ¡fico com tons naturais. Ãudio: ambiente natural com sons de movimento sutis. Filmado em estilo documental, 4K, texturas detalhadas.",
                "prompt_veo3": "Movimento natural e cinematogrÃ¡fico do sujeito. Close-up com lente 85mm f/1.8, shallow depth of field. Three-point lighting setup profissional. Color grading com tons cinematogrÃ¡ficos. Audio design: ambiente natural com elementos sonoros sincronizados. Hyper-realistic, 4K, estilo de commercial high-end.",
                "prompt_economico": "AnimaÃ§Ã£o suave e natural da imagem. Medium shot. IluminaÃ§Ã£o natural. Movimento realista. Qualidade cinematogrÃ¡fica.",
                "cinematic_details": {
                    "subject_action": "Movimento natural e realista do sujeito",
                    "camera_work": "Medium shot, cÃ¢mera estÃ¡tica",
                    "lighting": "Soft natural light",
                    "audio_design": "Ambiente natural com sons sutis",
                    "style": "CinematogrÃ¡fico, 4K, texturas detalhadas"
                },
                "tips": "AnÃ¡lise automÃ¡tica nÃ£o disponÃ­vel. VocÃª pode editar o prompt conforme necessÃ¡rio para seu vÃ­deo especÃ­fico."
            }
            
            # Clean up temp file
            if os.path.exists(temp_path):
                os.remove(temp_path)
            
            return {
                "success": True,
                "analysis": analysis_data,
                "warning": "AnÃ¡lise automÃ¡tica falhou. Usando configuraÃ§Ãµes padrÃ£o."
            }
        
        # Clean up temp file
        os.remove(temp_path)
        
        # Parse response
        import json
        analysis_data = json.loads(response.strip('```json').strip('```').strip())
        
        # Function to sanitize all prompts in analysis
        def sanitize_analysis_prompts(data):
            """Clean all prompts in analysis data"""
            problematic_words = {
                'ameaÃ§ador': 'impressionante',
                'ameaÃ§adora': 'impressionante',
                'ameaÃ§adoramente': 'impressionantemente',
                'assustador': 'surpreendente',
                'assustadora': 'surpreendente',
                'violento': 'intenso',
                'violenta': 'intensa',
                'afiados': 'visÃ­veis',
                'afiado': 'visÃ­vel',
                'afiada': 'visÃ­vel',
                'ataque': 'aproximaÃ§Ã£o',
                'atacar': 'se aproximar',
                'atacando': 'se aproximando',
                'medo': 'admiraÃ§Ã£o',
                'terror': 'impacto',
                'pÃ¢nico': 'reaÃ§Ã£o intensa',
                'sangue': 'efeito dramÃ¡tico',
                'morte': 'drama',
                'agressiv': 'energÃ©tic'
            }
            
            # Clean all string fields recursively
            def clean_text(text):
                if not isinstance(text, str):
                    return text
                cleaned = text
                
                # Remove problematic words
                for word, replacement in problematic_words.items():
                    cleaned = cleaned.replace(word, replacement)
                
                # Remove facial fidelity instructions (triggers deepfake detection)
                import re
                fidelity_patterns = [
                    r'\[Manter a identidade facial.*?\]',
                    r'\[.*?NÃƒO DEVEM ser alterados.*?\]',
                    r'\[.*?preservando 100%.*?\]',
                    r'Manter a identidade facial.*?caracterÃ­sticas fÃ­sicas\.',
                    r'Os rostos.*?NÃƒO DEVEM.*?substituÃ­dos\.',
                    r'preservando 100% da fidelidade.*?\.'
                ]
                
                for pattern in fidelity_patterns:
                    cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE | re.DOTALL)
                
                # Clean up extra spaces
                cleaned = re.sub(r'\s+', ' ', cleaned).strip()
                
                return cleaned
            
            def clean_dict(d):
                if isinstance(d, dict):
                    return {k: clean_dict(v) for k, v in d.items()}
                elif isinstance(d, list):
                    return [clean_dict(item) for item in d]
                elif isinstance(d, str):
                    return clean_text(d)
                return d
            
            return clean_dict(data)
        
        # Sanitize the analysis data
        analysis_data = sanitize_analysis_prompts(analysis_data)

        # Save to database
        analysis = ImageAnalysis(
            image_url=request.image_url,
            analysis=json.dumps(analysis_data),
            suggested_model=analysis_data.get('recommended_model_premium', 'veo3')
        )

        doc = analysis.model_dump()
        doc['timestamp'] = doc['timestamp'].isoformat()
        await database.insert_image_analysis(doc)
        
        return {
            "success": True,
            "analysis": analysis_data
        }
    except Exception as e:
        logger.error(f"Error analyzing image: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@api_router.get("/audio/voices")
async def get_voices():
    """Get available ElevenLabs voices"""
    try:
        voices_response = elevenlabs_client.voices.get_all()
        
        # Filter for child voices or Portuguese
        voices = []
        for voice in voices_response.voices:
            voice_data = {
                "voice_id": voice.voice_id,
                "name": voice.name,
                "category": voice.category if hasattr(voice, 'category') else "general",
                "labels": voice.labels if hasattr(voice, 'labels') else {}
            }
            voices.append(voice_data)
        
        return {"success": True, "voices": voices}
    except Exception as e:
        logger.error(f"Error fetching voices: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@api_router.post("/audio/generate")
async def generate_audio(request: GenerateAudioRequest):
    """Generate audio with ElevenLabs"""
    try:
        from elevenlabs import VoiceSettings
        
        voice_settings = VoiceSettings(
            stability=request.stability,
            similarity_boost=request.similarity_boost,
            style=request.style,
            use_speaker_boost=True
        )
        
        audio_generator = elevenlabs_client.text_to_speech.convert(
            text=request.text,
            voice_id=request.voice_id,
            model_id="eleven_multilingual_v2",
            voice_settings=voice_settings
        )
        
        # Collect audio data
        audio_data = b""
        for chunk in audio_generator:
            audio_data += chunk
        
        # Convert to base64
        audio_b64 = base64.b64encode(audio_data).decode()
        audio_url = f"data:audio/mpeg;base64,{audio_b64}"
        
        # Estimate duration (rough estimate based on text length)
        duration = len(request.text) * 0.05  # ~50ms per character
        
        # Estimate cost (ElevenLabs pricing ~$0.30 per 1000 chars)
        cost = (len(request.text) / 1000) * 0.30
        
        # Save to database
        audio = AudioGeneration(
            audio_url=audio_url,
            source="generated",
            duration=duration,
            text=request.text,
            voice_id=request.voice_id,
            voice_settings=voice_settings.model_dump(),
            cost=cost
        )

        doc = audio.model_dump()
        doc['timestamp'] = doc['timestamp'].isoformat()
        await database.insert_audio_generation(doc)

        # Track usage
        usage = TokenUsage(
            service="elevenlabs",
            operation="text_to_speech",
            cost=cost,
            details={"characters": len(request.text)}
        )
        usage_doc = usage.model_dump()
        usage_doc['timestamp'] = usage_doc['timestamp'].isoformat()
        await database.insert_token_usage(usage_doc)
        
        return {
            "success": True,
            "audio_id": audio.id,
            "audio_url": audio_url,
            "duration": duration,
            "cost": cost
        }
    except Exception as e:
        logger.error(f"Error generating audio: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@api_router.post("/video/estimate-cost")
async def estimate_cost(request: EstimateCostRequest):
    """Estimate video generation cost"""
    try:
        cost = 0.0
        
        if request.mode == "economico":
            # Modelos gratuitos do HuggingFace
            cost = 0.0
        else:
            # Modelos premium (FAL.AI)
            if request.model == "veo3":
                if request.with_audio:
                    cost = request.duration * 0.40
                else:
                    cost = request.duration * 0.20
            elif request.model == "sora2":
                cost = request.duration * 0.10
            elif request.model == "wav2lip":
                # Wav2lip pricing (estimate)
                cost = request.duration * 0.05
        
        return {
            "success": True,
            "estimated_cost": round(cost, 2),
            "model": request.model,
            "mode": request.mode,
            "duration": request.duration,
            "with_audio": request.with_audio,
            "is_free": request.mode == "economico"
        }
    except Exception as e:
        logger.error(f"Error estimating cost: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@api_router.post("/video/test-prompt")
async def test_prompt(request: dict):
    """Test endpoint to see what prompt is received"""
    logger.info(f"ðŸ” TEST PROMPT RECEIVED: {request.get('prompt', 'NO PROMPT')}")
    return {"received_prompt": request.get('prompt', 'NO PROMPT')}

@api_router.post("/video/generate")
async def generate_video(request: GenerateVideoRequest):
    """Generate video with selected model (Premium or EconÃ´mico)"""
    try:
        video_id = str(uuid.uuid4())
        
        # Function to sanitize prompt for content policy
        def sanitize_prompt(prompt):
            """Remove ALL potentially problematic content that triggers FAL.AI filters"""
            import re
            
            # STEP 1: Remove ALL mentions of facial fidelity/identity/preservation
            # These trigger deepfake detection
            fidelity_removal_patterns = [
                r'\[Manter[^\]]*\]',  # Remove all [Manter...] blocks
                r'\[.*?identidade.*?\]',  # Any block mentioning identity
                r'\[.*?fidelidade.*?\]',  # Any block mentioning fidelity
                r'\[.*?NÃƒO DEVEM.*?\]',  # Any block with NÃƒO DEVEM
                r'\[.*?preserv.*?\]',  # Any block mentioning preserve
                r'[Mm]anter.*?identidade[^.]*\.',  # Sentences about maintaining identity
                r'[Pp]reserv.*?fidelidade[^.]*\.',  # Sentences about preserving fidelity
                r'[^.]*?expressÃµes faciais[^.]*?fidelidade[^.]*?\.',  # Any sentence with facial expressions + fidelity
                r'[Aa]s expressÃµes faciais devem ser preservadas[^.]*?\.',  # Specific phrase
                r'com alta fidelidade',  # Remove "with high fidelity" mentions
                r'devem ser preservadas[^.]*?fidelidade[^.]*?',  # Must be preserved with fidelity
                r'alta fidelidade[^.]*?',  # Remove "high fidelity"
                r'[^.]*?preservadas com alta fidelidade[^.]*?\.',  # Preserved with high fidelity sentence
                r'expressÃµes faciais[^.]*?alta fidelidade[^.]*?\.',  # Facial expressions with high fidelity
            ]
            
            sanitized = prompt
            for pattern in fidelity_removal_patterns:
                sanitized = re.sub(pattern, '', sanitized, flags=re.IGNORECASE | re.DOTALL)
            
            # STEP 2: Remove violent/threatening words
            word_replacements = [
                (r'ameaÃ§ador(a|amente|es)?', 'impressionante'),
                (r'assustador(a|es)?', 'surpreendente'),
                (r'violento?(a|s)?', 'intenso'),
                (r'afiado?(a|s)?', 'visÃ­vel'),
                (r'ataca(r|ndo|m)?', 'aproxima'),
                (r'ataque', 'aproximaÃ§Ã£o'),
                (r'medo', 'admiraÃ§Ã£o'),
                (r'terror', 'impacto'),
                (r'pÃ¢nico', 'intensidade'),
                (r'perigoso?(a|s)?', 'impressionante')
            ]
            
            for pattern, replacement in word_replacements:
                sanitized = re.sub(pattern, replacement, sanitized, flags=re.IGNORECASE)
            
            # STEP 3: Clean up extra spaces and formatting
            sanitized = re.sub(r'\s+', ' ', sanitized)  # Multiple spaces to single
            sanitized = re.sub(r'\.\s*\.', '.', sanitized)  # Double periods
            sanitized = re.sub(r'\s+([,.])', r'\1', sanitized)  # Space before punctuation
            sanitized = sanitized.strip()
            
            return sanitized
        
        # Sanitize prompt
        original_prompt = request.prompt
        sanitized_prompt = sanitize_prompt(request.prompt)
        
        if original_prompt != sanitized_prompt:
            logger.warning("âš ï¸ PROMPT SANITIZED!")
            logger.warning(f"REMOVED {len(original_prompt) - len(sanitized_prompt)} characters")
            logger.warning(f"CLEAN PROMPT: {sanitized_prompt[:300]}")
        else:
            logger.info(f"âœ… Prompt clean: {sanitized_prompt[:100]}")
        
        # Calculate cost based on mode
        cost = 0.0
        if request.mode == "premium":
            if request.model == "veo3":
                cost = request.duration * (0.40 if request.audio_url else 0.20)
            elif request.model == "sora2":
                cost = request.duration * 0.10
            elif request.model == "wav2lip":
                cost = request.duration * 0.05
        # Modo econÃ´mico Ã© gratuito
        
        # Save initial record
        video = VideoGeneration(
            id=video_id,
            image_id=request.image_url,
            audio_id=request.audio_url,
            model=request.model,
            mode=request.mode,
            prompt=request.prompt,
            duration=request.duration,
            estimated_cost=cost,
            status="processing"
        )

        doc = video.model_dump()
        doc['timestamp'] = doc['timestamp'].isoformat()
        await database.insert_video_generation(doc)
        
        # Generate video based on model and mode
        result_url = None
        
        if request.mode == "premium":
            # Premium models via FAL.AI
            if request.model == "veo3":
                import asyncio
                # Veo 3 only accepts "8s" duration format
                veo3_duration = "8s"  # Fixed duration for Veo 3
                handler = fal_client.submit(
                    "fal-ai/veo3.1/image-to-video",
                    arguments={
                        "image_url": request.image_url,
                        "prompt": sanitized_prompt,  # Use sanitized prompt
                        "duration": veo3_duration
                    }
                )
                # Run in executor to avoid blocking
                result = await asyncio.get_event_loop().run_in_executor(
                    None, handler.get
                )
                result_url = result.get('video', {}).get('url')
                
            elif request.model == "sora2":
                import asyncio
                handler = fal_client.submit(
                    "fal-ai/sora-2/image-to-video",
                    arguments={
                        "image_url": request.image_url,
                        "prompt": sanitized_prompt  # Use sanitized prompt
                    }
                )
                # Run in executor to avoid blocking
                result = await asyncio.get_event_loop().run_in_executor(
                    None, handler.get
                )
                result_url = result.get('video', {}).get('url')
                
            elif request.model == "wav2lip":
                if not request.audio_url:
                    raise HTTPException(status_code=400, detail="Audio URL required for Wav2lip")
                
                import asyncio
                handler = fal_client.submit(
                    "fal-ai/wav2lip",
                    arguments={
                        "face_url": request.image_url,
                        "audio_url": request.audio_url
                    }
                )
                # Run in executor to avoid blocking
                result = await asyncio.get_event_loop().run_in_executor(
                    None, handler.get
                )
                result_url = result.get('video', {}).get('url')
        
        elif request.mode == "economico":
            # Free models via HuggingFace Spaces
            if request.model == "open-sora":
                try:
                    # Use HuggingFace Open-Sora Space
                    hf_client = Client("hpcai-tech/Open-Sora")
                    result = hf_client.predict(
                        prompt=request.prompt,
                        image=request.image_url,
                        api_name="/predict"
                    )
                    result_url = result
                except Exception as e:
                    logger.error(f"Error with Open-Sora: {str(e)}")
                    raise HTTPException(status_code=503, detail=f"Modelo Open-Sora temporariamente indisponÃ­vel. Erro: {str(e)}")
                    
            elif request.model == "wav2lip-free":
                if not request.audio_url:
                    raise HTTPException(status_code=400, detail="Audio URL required for Wav2lip")
                
                try:
                    # Use HuggingFace Wav2Lip Space (procurar space pÃºblico disponÃ­vel)
                    # Nota: Pode variar dependendo do space disponÃ­vel
                    hf_client = Client("fffiloni/Wav2Lip")
                    result = hf_client.predict(
                        image=request.image_url,
                        audio=request.audio_url,
                        api_name="/predict"
                    )
                    result_url = result
                except Exception as e:
                    logger.error(f"Error with Wav2Lip Free: {str(e)}")
                    raise HTTPException(status_code=503, detail=f"Modelo Wav2Lip Free temporariamente indisponÃ­vel. Erro: {str(e)}")
        
        # Update record
        await database.update_video_generation(video_id, {
            "status": "completed",
            "result_url": result_url,
            "cost": cost
        })

        # Track usage (only for paid services)
        if cost > 0:
            usage = TokenUsage(
                service="fal_ai",
                operation=f"video_generation_{request.model}",
                cost=cost,
                details={"duration": request.duration, "model": request.model, "mode": request.mode}
            )
            usage_doc = usage.model_dump()
            usage_doc['timestamp'] = usage_doc['timestamp'].isoformat()
            await database.insert_token_usage(usage_doc)
        
        return {
            "success": True,
            "video_id": video_id,
            "video_url": result_url,
            "cost": cost,
            "mode": request.mode,
            "is_free": request.mode == "economico"
        }
        
    except Exception as e:
        logger.error(f"Error generating video: {str(e)}")
        
        # Check if it's a content policy violation
        error_message = str(e)
        if 'content_policy_violation' in error_message or 'content checker' in error_message:
            friendly_message = """âš ï¸ PolÃ­tica de ConteÃºdo: O prompt contÃ©m termos que foram bloqueados pela polÃ­tica de conteÃºdo da IA.

Dicas para resolver:
â€¢ Evite palavras como: ameaÃ§ador, violento, ataque, sangue, armas
â€¢ Use palavras neutras: impressionante, surpreendente, dramÃ¡tico
â€¢ Foque na descriÃ§Ã£o visual sem conotaÃ§Ã£o violenta

Exemplo: Em vez de "T-Rex ameaÃ§ador rugindo", use "T-Rex impressionante com boca aberta"."""
            error_code = "CONTENT_POLICY"
        else:
            friendly_message = f"Erro ao gerar vÃ­deo: {error_message}"
            error_code = "GENERATION_ERROR"
        
        # Update record with error
        await database.update_video_generation(video_id, {
            "status": "failed",
            "error": error_message
        })
        
        raise HTTPException(
            status_code=422 if error_code == "CONTENT_POLICY" else 500,
            detail={
                "error_code": error_code,
                "message": friendly_message,
                "original_error": error_message if error_code != "CONTENT_POLICY" else None
            }
        )

@api_router.post("/auth/verify")
async def verify_password(request: VerifyPasswordRequest):
    """Verify admin password"""
    admin_password = os.environ.get('ADMIN_PASSWORD', 'mauricio123')
    
    if request.password == admin_password:
        return {"success": True, "message": "Password verified"}
    else:
        raise HTTPException(status_code=401, detail="Invalid password")

@api_router.get("/tokens/usage")
async def get_token_usage():
    """Get token usage statistics"""
    try:
        # Get all usage records
        usage_records = await database.get_token_usage(limit=1000)

        # Calculate totals
        total_spent = sum(record.get('cost', 0) for record in usage_records)

        # Group by service
        by_service = {}
        for record in usage_records:
            service = record.get('service', 'unknown')
            if service not in by_service:
                by_service[service] = 0
            by_service[service] += record.get('cost', 0)

        # Get recent operations (last 10)
        recent = sorted(usage_records, key=lambda x: x.get('timestamp', ''), reverse=True)[:10]

        # Get balances
        balances = await database.get_all_api_balances()

        # Calculate remaining balances
        balance_info = {}
        for balance in balances:
            service = balance.get('service')
            initial = balance.get('initial_balance', 0)
            spent = by_service.get(service, 0)
            remaining = initial - spent
            balance_info[service] = {
                "initial": round(initial, 2),
                "spent": round(spent, 2),
                "remaining": round(remaining, 2)
            }

        return {
            "success": True,
            "total_spent": round(total_spent, 2),
            "by_service": {k: round(v, 2) for k, v in by_service.items()},
            "recent_operations": recent,
            "balances": balance_info
        }
    except Exception as e:
        logger.error(f"Error getting token usage: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@api_router.post("/tokens/balance")
async def update_balance(request: UpdateBalanceRequest):
    """Update or create API balance"""
    try:
        await database.upsert_api_balance(request.service, request.initial_balance)

        return {
            "success": True,
            "message": f"Saldo atualizado para {request.service}"
        }
    except Exception as e:
        logger.error(f"Error updating balance: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@api_router.get("/tokens/balances")
async def get_balances():
    """Get all API balances"""
    try:
        # Get all balances
        balances = await database.get_all_api_balances()

        # Get usage
        usage_records = await database.get_token_usage(limit=1000)

        # Group by service
        by_service = {}
        for record in usage_records:
            service = record.get('service', 'unknown')
            if service not in by_service:
                by_service[service] = 0
            by_service[service] += record.get('cost', 0)

        # Calculate remaining for each service
        result = {}
        for balance in balances:
            service = balance.get('service')
            initial = balance.get('initial_balance', 0)
            spent = by_service.get(service, 0)
            remaining = initial - spent
            result[service] = {
                "initial": round(initial, 2),
                "spent": round(spent, 2),
                "remaining": round(remaining, 2)
            }

        return {
            "success": True,
            "balances": result
        }
    except Exception as e:
        logger.error(f"Error getting balances: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@api_router.get("/gallery/items")
async def get_gallery_items():
    """Get all generated videos and audios for gallery"""
    try:
        # Get all videos (only completed ones)
        videos = await database.get_video_generations(status="completed", limit=100)

        # Get all audios
        audios = await database.get_audio_generations(limit=100)

        # Get all images
        images = await database.get_image_analyses(limit=100)

        return {
            "success": True,
            "videos": videos,
            "audios": audios,
            "images": images
        }
    except Exception as e:
        logger.error(f"Error getting gallery items: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@api_router.delete("/gallery/video/{video_id}")
async def delete_video(video_id: str):
    """Delete a video from gallery"""
    try:
        deleted = await database.delete_video_generation(video_id)
        if deleted:
            return {"success": True, "message": "VÃ­deo deletado"}
        else:
            raise HTTPException(status_code=404, detail="VÃ­deo nÃ£o encontrado")
    except Exception as e:
        logger.error(f"Error deleting video: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@api_router.delete("/gallery/audio/{audio_id}")
async def delete_audio(audio_id: str):
    """Delete an audio from gallery"""
    try:
        deleted = await database.delete_audio_generation(audio_id)
        if deleted:
            return {"success": True, "message": "Ãudio deletado"}
        else:
            raise HTTPException(status_code=404, detail="Ãudio nÃ£o encontrado")
    except Exception as e:
        logger.error(f"Error deleting audio: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@api_router.delete("/gallery/image/{image_id}")
async def delete_image(image_id: str):
    """Delete an image analysis from gallery"""
    try:
        deleted = await database.delete_image_analysis(image_id)
        if deleted:
            return {"success": True, "message": "Imagem deletada"}
        else:
            raise HTTPException(status_code=404, detail="Imagem nÃ£o encontrada")
    except Exception as e:
        logger.error(f"Error deleting image: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== IMAGE GENERATION ENDPOINTS ====================

@api_router.post("/images/generate")
async def generate_image_with_nano_banana(
    prompt: str = Form(...),
    reference_image: Optional[UploadFile] = File(None)
):
    """Generate image using Gemini 2.5 Flash Image (Nano Banana) with optional reference image"""
    file_contents = []  # Initialize outside try block for finally cleanup
    
    try:
        logger.info(f"ðŸŽ¨ Generating image with prompt: {prompt[:100]}...")
        
        # Use FAL.AI for image generation with FLUX model
        import fal_client
        import base64
        import tempfile
        import requests

        # Prepare arguments for FAL
        fal_arguments = {
            "prompt": prompt,
            "image_size": "landscape_4_3",  # Options: square, landscape_4_3, landscape_16_9, portrait_4_3, portrait_16_9
            "num_inference_steps": 28,
            "guidance_scale": 3.5,
            "num_images": 1,
            "enable_safety_checker": True
        }
        
        # If reference image is provided, upload it and use in image_url parameter
        if reference_image:
            try:
                # Read image file
                image_bytes = await reference_image.read()
                
                logger.info(f"ðŸ“Ž Reference image provided: {reference_image.filename}")
                
                # Upload reference image to cloudinary first
                import io
                upload_result = cloudinary.uploader.upload(
                    io.BytesIO(image_bytes),
                    folder="reference_images",
                    resource_type="image"
                )
                reference_image_url = upload_result['secure_url']
                logger.info(f"âœ… Reference image uploaded: {reference_image_url}")
                
                # Add reference image to FAL arguments
                fal_arguments["image_url"] = reference_image_url
                fal_arguments["prompt"] = f"Using the reference image as inspiration, {prompt}"
                
            except Exception as img_error:
                logger.warning(f"Failed to process reference image: {str(img_error)}")
        
        # Generate image with FAL.AI using FLUX model
        import asyncio
        try:
            logger.info(f"ðŸŽ¨ Calling FAL.AI FLUX model with arguments: {fal_arguments}")
            
            # Use FAL.AI FLUX model for image generation
            def generate_with_fal():
                return fal_client.subscribe(
                    "fal-ai/flux/dev",  # FLUX.1 Dev model
                    arguments=fal_arguments,
                    with_logs=False
                )
            
            # Run synchronous FAL call in executor
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(None, generate_with_fal)
            
            logger.info(f"âœ… FAL.AI generation completed")
            logger.info(f"ðŸ” Result keys: {result.keys() if isinstance(result, dict) else 'Not a dict'}")
            
            # Extract image URL from result
            if 'images' in result and len(result['images']) > 0:
                # FLUX returns a list of image objects with 'url' field
                image_url = result['images'][0]['url']
                logger.info(f"âœ… Image generated: {image_url}")
                
                # Download image and upload to Cloudinary for permanent storage
                try:
                    response = requests.get(image_url, timeout=30)
                    response.raise_for_status()
                    
                    upload_result = cloudinary.uploader.upload(
                        io.BytesIO(response.content),
                        folder="generated_images",
                        resource_type="image"
                    )
                    image_url = upload_result['secure_url']
                    cloudinary_id = upload_result['public_id']
                    logger.info(f"âœ… Image uploaded to Cloudinary: {cloudinary_id}")
                except Exception as cloudinary_error:
                    logger.warning(f"Cloudinary upload failed: {str(cloudinary_error)}, using FAL URL")
                    cloudinary_id = None
            else:
                raise HTTPException(status_code=500, detail="No image generated by FAL.AI")
            
            image_id = str(uuid.uuid4())
            
            # Save to database
            generated_image = GeneratedImage(
                id=image_id,
                prompt=prompt,
                image_url=image_url,
                cost=0.025
            )

            doc = generated_image.model_dump()
            doc['timestamp'] = doc['timestamp'].isoformat()
            if cloudinary_id:
                doc['cloudinary_id'] = cloudinary_id
            await database.insert_generated_image(doc)

            # Track cost (FLUX.1 Dev pricing)
            usage = TokenUsage(
                service="FAL.AI FLUX Image Generation",
                operation="generate_image",
                cost=0.025,  # Approximate cost per image with FLUX.1 Dev
                details={
                    "prompt_length": len(prompt), 
                    "model": "fal-ai/flux/dev",
                    "has_reference_image": reference_image is not None,
                    "image_size": fal_arguments["image_size"]
                }
            )
            usage_doc = usage.model_dump()
            usage_doc['timestamp'] = usage_doc['timestamp'].isoformat()
            await database.insert_token_usage(usage_doc)
            
            return {
                "success": True,
                "image_id": image_id,
                "image_url": image_url,
                "prompt": prompt,
                "cost": 0.025
            }
            
        except asyncio.TimeoutError:
            logger.error("Image generation timed out")
            raise HTTPException(status_code=504, detail="Image generation timed out. Please try again.")
            
    except Exception as e:
        logger.error(f"Error generating image: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@api_router.get("/images/generated")
async def get_generated_images():
    """Get all generated images"""
    try:
        images = await database.get_generated_images(limit=100)

        return {
            "success": True,
            "images": images
        }
    except Exception as e:
        logger.error(f"Error fetching generated images: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@api_router.delete("/images/generated/{image_id}")
async def delete_generated_image(image_id: str):
    """Delete a generated image"""
    try:
        deleted = await database.delete_generated_image(image_id)

        if deleted:
            return {"success": True, "message": "Generated image deleted"}
        else:
            raise HTTPException(status_code=404, detail="Image not found")
    except Exception as e:
        logger.error(f"Error deleting generated image: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== ROOT ENDPOINTS ====================

@app.get("/")
async def root():
    """API Root - Returns basic information"""
    return {
        "name": "Talking Photo Generator API",
        "version": "1.0.0",
        "status": "running",
        "endpoints": {
            "health": "/health",
            "api": "/api",
            "docs": "/docs",
            "redoc": "/redoc"
        }
    }

@app.get("/health")
async def health_check():
    """Health check endpoint for monitoring"""
    try:
        # Check database connection
        await database.init_db()
        
        return {
            "status": "healthy",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "services": {
                "api": "ok",
                "database": "ok",
                "cloudinary": "configured" if os.environ.get('CLOUDINARY_CLOUD_NAME') else "not_configured",
                "gemini": "configured" if os.environ.get('GEMINI_KEY') else "not_configured",
                "elevenlabs": "configured" if os.environ.get('ELEVENLABS_KEY') else "not_configured",
                "fal": "configured" if os.environ.get('FAL_KEY') else "not_configured"
            }
        }
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

# Include the router in the main app
app.include_router(api_router)

app.add_middleware(
    CORSMiddleware,
    allow_credentials=True,
    allow_origins=os.environ.get('CORS_ORIGINS', '*').split(','),
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("startup")
async def startup_db():
    """Initialize SQLite database on startup"""
    await database.init_db()
    logger.info("âœ… SQLite database initialized successfully")